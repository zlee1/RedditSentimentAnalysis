{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies\n",
    "#!pip install praw\n",
    "#!pip install psaw\n",
    "#!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nominated-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warning messages\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "refined-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load client_id, secret_id, and user_agent\n",
    "with open('info.json') as f:\n",
    "     info = json.load(f)\n",
    "        \n",
    "info = dict(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laughing-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit and PushshiftAPI instances\n",
    "reddit = praw.Reddit(client_id=info[\"client_id\"], user_agent=info[\"user_agent\"], client_secret=info[\"client_secret\"])\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70e2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results of a search in a DataFrame\n",
    "\"\"\"\n",
    "subm_dicts = [{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in api.search_submissions(subreddit='stocks', q=\"TWTR\", filter=['url','author', 'title', 'subreddit'], limit=100)]\n",
    "df = pd.DataFrame(subm_dicts)\n",
    "df\n",
    "\"\"\"\n",
    "_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5295a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-11-07</th>\n",
       "      <td>45.099998</td>\n",
       "      <td>50.090000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>117701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-08</th>\n",
       "      <td>45.930000</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>40.689999</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>27925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-11</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>39.400002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>16113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-12</th>\n",
       "      <td>43.660000</td>\n",
       "      <td>43.779999</td>\n",
       "      <td>41.830002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>6316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-13</th>\n",
       "      <td>41.029999</td>\n",
       "      <td>42.869999</td>\n",
       "      <td>40.759998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>8688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-04</th>\n",
       "      <td>47.869999</td>\n",
       "      <td>51.369999</td>\n",
       "      <td>46.860001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>268465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>53.849998</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>50.560001</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>217520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-06</th>\n",
       "      <td>50.040001</td>\n",
       "      <td>52.869999</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>159034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-07</th>\n",
       "      <td>50.470001</td>\n",
       "      <td>51.639999</td>\n",
       "      <td>46.549999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>120844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-08</th>\n",
       "      <td>47.299999</td>\n",
       "      <td>48.439999</td>\n",
       "      <td>45.830002</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>83262600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2013-11-07  45.099998  50.090000  44.000000  44.900002  44.900002  117701600\n",
       "2013-11-08  45.930000  46.939999  40.689999  41.650002  41.650002   27925300\n",
       "2013-11-11  40.500000  43.000000  39.400002  42.900002  42.900002   16113900\n",
       "2013-11-12  43.660000  43.779999  41.830002  41.900002  41.900002    6316700\n",
       "2013-11-13  41.029999  42.869999  40.759998  42.599998  42.599998    8688300\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2022-04-04  47.869999  51.369999  46.860001  49.970001  49.970001  268465400\n",
       "2022-04-05  53.849998  54.570000  50.560001  50.980000  50.980000  217520100\n",
       "2022-04-06  50.040001  52.869999  49.299999  50.770000  50.770000  159034700\n",
       "2022-04-07  50.470001  51.639999  46.549999  48.029999  48.029999  120844100\n",
       "2022-04-08  47.299999  48.439999  45.830002  46.230000  46.230000   83262600\n",
       "\n",
       "[2120 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get historical stock data for a ticker\n",
    "twtr = yf.download('TWTR', progress=True)\n",
    "twtr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cee07",
   "metadata": {},
   "source": [
    "## Processing Tickers\n",
    "\n",
    "This initial thought process is not great. I decided that looking for specific words wwould not be a good idea, as it takes much of the context out of the comment. (Something that briefly mentions TSLA, but is actually talking about how great MSFT is would be useless in predicting TSLA stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5ce231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At close, calculate the real and percent change since last close\n",
    "def get_diff(ticker_data):\n",
    "    df = ticker_data.copy()\n",
    "    real = []\n",
    "    percent = []\n",
    "    for index, row in df.reset_index().iterrows():\n",
    "        if(index == 0):\n",
    "            real.append(0)\n",
    "            percent.append(0)\n",
    "        else:\n",
    "            real.append(row[\"Close\"]-df.iloc[index-1][\"Close\"])\n",
    "            percent.append(real[-1]/df.iloc[index-1][\"Close\"])\n",
    "    return real, percent\n",
    "\n",
    "# Get the reddit posts that mention a certain ticker n days before a large change in stock price\n",
    "def get_pre_change_posts(ticker, ticker_gain, days=1, limit=1000, subreddit=\"stocks,stockmarket,stocksandtrading,daytrading,investing,stocks_picks,stockstobuytoday\"):\n",
    "    df = None\n",
    "    for index, row in ticker_gain.iterrows():\n",
    "        start_date = datetime.fromtimestamp(row[\"Date\"].timestamp()) + timedelta(hours=6, days=-days)\n",
    "        end_date = datetime.fromtimestamp(row[\"Date\"].timestamp()) + timedelta(hours=6)\n",
    "        \n",
    "        # TODO: Check whether comments would be better than submissions\n",
    "        \n",
    "        submissions = api.search_comments(after=start_date, before=end_date, q=ticker, subreddit=subreddit, filter=['url','author', 'title', 'subreddit'], limit=limit)\n",
    "        if(df is None):\n",
    "            df = pd.DataFrame([{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in submissions])\n",
    "        else:\n",
    "            df = df.append([{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in submissions], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def word_counts(df, column=\"body\", min_letters=3):\n",
    "    counts = {}\n",
    "    for i in list(df[column]):\n",
    "        for j in i.split(\" \"):\n",
    "            j = ''.join(k for k in j if k.isalnum())\n",
    "            # Exclude words that are likely tickers\n",
    "            if(j == j.upper() and len(j) > 1 and len(j) <= 5):\n",
    "                pass\n",
    "            elif(len(j) < 3):\n",
    "                pass\n",
    "            elif(j not in counts.keys()):\n",
    "                counts[j.lower()] = 1\n",
    "            else:\n",
    "                counts.update({j.lower():counts.get(j.lower())+1})\n",
    "    return counts\n",
    "\n",
    "def remove_shared_keys(dict_a, dict_b, cutoff=2):\n",
    "    a = dict_a.copy()\n",
    "    b = dict_b.copy()\n",
    "    \n",
    "    rm_a = []\n",
    "    rm_b = []\n",
    "    for i in a:\n",
    "        if(i in b):\n",
    "            if(b.get(i) > 2*a.get(i)):\n",
    "                rm_a.append(i)\n",
    "            elif(b.get(i) < 2*a.get(i)):\n",
    "                rm_b.append(i)\n",
    "            else:\n",
    "                rm_a.append(i)\n",
    "                rm_b.append(i)\n",
    "    for i in rm_a:\n",
    "        a.pop(i)\n",
    "    for i in rm_b:\n",
    "        b.pop(i)\n",
    "    return a, b\n",
    "\n",
    "def remove_infrequent_words(dict_a, min_count=2):\n",
    "    d = dict_a.copy()\n",
    "    \n",
    "    to_remove = []\n",
    "    for i, x in d.items():\n",
    "        if(x < min_count):\n",
    "            to_remove.append(i)\n",
    "            \n",
    "    for i in to_remove:\n",
    "        d.pop(i)\n",
    "        \n",
    "    return d\n",
    "\n",
    "# Generate information for a given ticker\n",
    "def process_ticker(ticker, gain_cutoff=0.05, loss_cutoff=0.05, limit=100, days=1):\n",
    "    try:\n",
    "        ticker_data = yf.download(ticker, progress=False)\n",
    "        ticker_data.reset_index(inplace=True)\n",
    "        real, percent = get_diff(ticker_data)\n",
    "\n",
    "        ticker_data[\"Real_Change\"] = real\n",
    "        ticker_data[\"Percent_Change\"] = percent\n",
    "\n",
    "        ticker_gain = ticker_data[ticker_data[\"Percent_Change\"] > gain_cutoff]\n",
    "        ticker_loss = ticker_data[ticker_data[\"Percent_Change\"] < -loss_cutoff]\n",
    "\n",
    "        pre_gain = get_pre_change_posts(ticker, ticker_gain, days, limit)\n",
    "        pre_loss = get_pre_change_posts(ticker, ticker_loss, days, limit)\n",
    "\n",
    "        gain_wc = dict(sorted(word_counts(pre_gain).items(), key=lambda x: x[1], reverse=True))\n",
    "        loss_wc = dict(sorted(word_counts(pre_loss).items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        gain_freq = remove_infrequent_words(gain_wc)\n",
    "        loss_freq = remove_infrequent_words(loss_wc)\n",
    "\n",
    "        gain_only, loss_only = remove_shared_keys(gain_freq, loss_freq)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}, {}\n",
    "    \n",
    "    return gain_only, loss_only\n",
    "\n",
    "#gain_only, loss_only = process_ticker(\"FB\")\n",
    "\n",
    "#gain_only\n",
    "\n",
    "#loss_only\n",
    "\"\"\"\n",
    "all_gain = []\n",
    "all_loss = []\n",
    "for ticker in [\"TWTR\", \"FB\", \"MSFT\", \"ADBE\", \"AAPL\", \"SNAP\", \"AMZN\", \"NCL\", \"DIS\", \"NFLX\"]:\n",
    "    gain_only, loss_only = process_ticker(ticker)\n",
    "    all_gain.append(gain_only)\n",
    "    all_loss.append(loss_only)\n",
    "    print(ticker)\n",
    "\n",
    "def combine_dict_list(list_of_dicts):\n",
    "    single_dict = {}\n",
    "    for d in list_of_dicts:\n",
    "        for i in d:\n",
    "            if(i not in single_dict):\n",
    "                single_dict[i] = d.get(i)\n",
    "            else:\n",
    "                single_dict.update({i:single_dict.get(i)+d.get(i)})\n",
    "    return dict(sorted(single_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "gain, loss = remove_shared_keys(combine_dict_list(all_gain), combine_dict_list(all_loss))\n",
    "\n",
    "gain\n",
    "\n",
    "loss\"\"\"\n",
    "_ = None # This is just to stop automatic output of block commented code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353ac1d",
   "metadata": {},
   "source": [
    "## Better Method (Probably)\n",
    "\n",
    "Instead of looking at posts/comments the day before and predicting whether the next day will close higher, this will be looking at the posts/comments from the previous day's close to the current day's open and predicting whether the close price will be higher than the open price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e3f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_change(data):\n",
    "    change = []\n",
    "    up = []\n",
    "    for index, row in data.iterrows():\n",
    "        change.append(row[\"Close\"]-row[\"Open\"])\n",
    "        up.append(int(change[-1] > 0))\n",
    "    return change, up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8123782",
   "metadata": {},
   "outputs": [],
   "source": [
    "change, up = daily_change(twtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de5a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "twtr[\"Daily_Change\"] = change\n",
    "twtr[\"Positive_Change\"] = up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d819a78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Change</th>\n",
       "      <th>Positive_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-11-07</th>\n",
       "      <td>45.099998</td>\n",
       "      <td>50.090000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>117701600</td>\n",
       "      <td>-0.199997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-08</th>\n",
       "      <td>45.930000</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>40.689999</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>27925300</td>\n",
       "      <td>-4.279999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-11</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>39.400002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>16113900</td>\n",
       "      <td>2.400002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-12</th>\n",
       "      <td>43.660000</td>\n",
       "      <td>43.779999</td>\n",
       "      <td>41.830002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>6316700</td>\n",
       "      <td>-1.759998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-13</th>\n",
       "      <td>41.029999</td>\n",
       "      <td>42.869999</td>\n",
       "      <td>40.759998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>8688300</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-04</th>\n",
       "      <td>47.869999</td>\n",
       "      <td>51.369999</td>\n",
       "      <td>46.860001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>268465400</td>\n",
       "      <td>2.100002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>53.849998</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>50.560001</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>217520100</td>\n",
       "      <td>-2.869999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-06</th>\n",
       "      <td>50.040001</td>\n",
       "      <td>52.869999</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>159034700</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-07</th>\n",
       "      <td>50.470001</td>\n",
       "      <td>51.639999</td>\n",
       "      <td>46.549999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>120844100</td>\n",
       "      <td>-2.440002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-08</th>\n",
       "      <td>47.299999</td>\n",
       "      <td>48.439999</td>\n",
       "      <td>45.830002</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>83262600</td>\n",
       "      <td>-1.070000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume  \\\n",
       "Date                                                                           \n",
       "2013-11-07  45.099998  50.090000  44.000000  44.900002  44.900002  117701600   \n",
       "2013-11-08  45.930000  46.939999  40.689999  41.650002  41.650002   27925300   \n",
       "2013-11-11  40.500000  43.000000  39.400002  42.900002  42.900002   16113900   \n",
       "2013-11-12  43.660000  43.779999  41.830002  41.900002  41.900002    6316700   \n",
       "2013-11-13  41.029999  42.869999  40.759998  42.599998  42.599998    8688300   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2022-04-04  47.869999  51.369999  46.860001  49.970001  49.970001  268465400   \n",
       "2022-04-05  53.849998  54.570000  50.560001  50.980000  50.980000  217520100   \n",
       "2022-04-06  50.040001  52.869999  49.299999  50.770000  50.770000  159034700   \n",
       "2022-04-07  50.470001  51.639999  46.549999  48.029999  48.029999  120844100   \n",
       "2022-04-08  47.299999  48.439999  45.830002  46.230000  46.230000   83262600   \n",
       "\n",
       "            Daily_Change  Positive_Change  \n",
       "Date                                       \n",
       "2013-11-07     -0.199997                0  \n",
       "2013-11-08     -4.279999                0  \n",
       "2013-11-11      2.400002                1  \n",
       "2013-11-12     -1.759998                0  \n",
       "2013-11-13      1.570000                1  \n",
       "...                  ...              ...  \n",
       "2022-04-04      2.100002                1  \n",
       "2022-04-05     -2.869999                0  \n",
       "2022-04-06      0.730000                1  \n",
       "2022-04-07     -2.440002                0  \n",
       "2022-04-08     -1.070000                0  \n",
       "\n",
       "[2120 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d128b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twtr = twtr.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82049eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_open_content(data, ticker, start_hour_diff=0, subreddit=\"stocks,stockmarket,stocksandtrading,daytrading,investing,stocks_picks,stockstobuytoday\", limit=100):\n",
    "\n",
    "    \n",
    "    new_col = []\n",
    "    for index, row in data.iterrows():\n",
    "        end_time = row.name + timedelta(hours=9, minutes=30)\n",
    "        start_time = end_time - timedelta(hours=17, minutes=30)\n",
    "        content = []\n",
    "        for i in api.search_comments(after=start_time, before=end_time, subreddit=subreddit, q=ticker, filter=['url','author', 'title', 'subreddit'], limit=limit):\n",
    "            for j in i.body.split(\".\"):\n",
    "                for k in j.split(\"\\n\"):\n",
    "                    content.append(k)\n",
    "        new_col.append(content)\n",
    "    \n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03f54ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes quite a while to run. Looking into how to speed it up. \n",
    "try:\n",
    "    twtr = pd.read_csv(\"data\\\\twtr.csv\")\n",
    "except:\n",
    "    twtr[\"Comments\"] = get_pre_open_content(twtr, \"TWTR\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4435fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Change</th>\n",
       "      <th>Positive_Change</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.510000</td>\n",
       "      <td>14.160000</td>\n",
       "      <td>14.430000</td>\n",
       "      <td>14.430000</td>\n",
       "      <td>18497400</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>18.549999</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>18.430000</td>\n",
       "      <td>18.430000</td>\n",
       "      <td>34586300</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>42.009998</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>41.529999</td>\n",
       "      <td>42.139999</td>\n",
       "      <td>42.139999</td>\n",
       "      <td>16087500</td>\n",
       "      <td>0.130001</td>\n",
       "      <td>1</td>\n",
       "      <td>['[deleted]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-13</td>\n",
       "      <td>51.990002</td>\n",
       "      <td>52.290001</td>\n",
       "      <td>51.450001</td>\n",
       "      <td>51.619999</td>\n",
       "      <td>51.619999</td>\n",
       "      <td>12306500</td>\n",
       "      <td>-0.370003</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-05</td>\n",
       "      <td>36.790001</td>\n",
       "      <td>37.230000</td>\n",
       "      <td>36.650002</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>11854300</td>\n",
       "      <td>0.209999</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I just randomly find them because I'm a mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>31.860001</td>\n",
       "      <td>32.389999</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>105112500</td>\n",
       "      <td>-1.110001</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>18.020000</td>\n",
       "      <td>18.020000</td>\n",
       "      <td>30988700</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>23.420000</td>\n",
       "      <td>23.629999</td>\n",
       "      <td>22.440001</td>\n",
       "      <td>22.959999</td>\n",
       "      <td>22.959999</td>\n",
       "      <td>44513700</td>\n",
       "      <td>-0.460001</td>\n",
       "      <td>0</td>\n",
       "      <td>['Remember, it can all be wiped out in a secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>40.549999</td>\n",
       "      <td>41.330002</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>41.080002</td>\n",
       "      <td>41.080002</td>\n",
       "      <td>12532500</td>\n",
       "      <td>0.530003</td>\n",
       "      <td>1</td>\n",
       "      <td>['ICLN 24%', '', 'ATVI 22%', '', 'KO 20%', '',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>24.320000</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>24.129999</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>63716800</td>\n",
       "      <td>0.550001</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2016-05-20  14.200000  14.510000  14.160000  14.430000  14.430000   \n",
       "1     2016-02-18  18.100000  18.549999  17.520000  18.430000  18.430000   \n",
       "2     2019-08-27  42.009998  42.689999  41.529999  42.139999  42.139999   \n",
       "3     2015-04-13  51.990002  52.290001  51.450001  51.619999  51.619999   \n",
       "4     2015-06-05  36.790001  37.230000  36.650002  37.000000  37.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "2115  2019-10-24  31.860001  32.389999  30.510000  30.750000  30.750000   \n",
       "2116  2016-11-04  17.520000  18.340000  17.500000  18.020000  18.020000   \n",
       "2117  2016-09-28  23.420000  23.629999  22.440001  22.959999  22.959999   \n",
       "2118  2020-08-26  40.549999  41.330002  40.000000  41.080002  41.080002   \n",
       "2119  2016-10-05  24.320000  25.250000  24.129999  24.870001  24.870001   \n",
       "\n",
       "         Volume  Daily_Change  Positive_Change  \\\n",
       "0      18497400      0.230000                1   \n",
       "1      34586300      0.330000                1   \n",
       "2      16087500      0.130001                1   \n",
       "3      12306500     -0.370003                0   \n",
       "4      11854300      0.209999                1   \n",
       "...         ...           ...              ...   \n",
       "2115  105112500     -1.110001                0   \n",
       "2116   30988700      0.500000                1   \n",
       "2117   44513700     -0.460001                0   \n",
       "2118   12532500      0.530003                1   \n",
       "2119   63716800      0.550001                1   \n",
       "\n",
       "                                               Comments  \n",
       "0                                                    []  \n",
       "1                                                    []  \n",
       "2                                         ['[deleted]']  \n",
       "3                                                    []  \n",
       "4     [\"I just randomly find them because I'm a mark...  \n",
       "...                                                 ...  \n",
       "2115                                                 []  \n",
       "2116                                                 []  \n",
       "2117  ['Remember, it can all be wiped out in a secon...  \n",
       "2118  ['ICLN 24%', '', 'ATVI 22%', '', 'KO 20%', '',...  \n",
       "2119                                                 []  \n",
       "\n",
       "[2120 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twtr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d761d1c",
   "metadata": {},
   "source": [
    "### Preparing text for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8e8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc2639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(data):\n",
    "    seqs = []\n",
    "    vals = []\n",
    "    for index, row in data.iterrows():\n",
    "        for comment in row[\"Comment_Sequences\"]:\n",
    "            if(comment != []):\n",
    "                seqs.append(list(comment))\n",
    "                vals.append(row[\"Positive_Change\"])\n",
    "    return seqs, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7e456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(data, train_proportion = 0.8, max_len=20):\n",
    "    tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "    \n",
    "    train = data[:int(data.shape[0]*train_proportion)]\n",
    "    test = data[int(data.shape[0]*train_proportion):]\n",
    "    \n",
    "    for comment in train.Comments:\n",
    "        tokenizer.fit_on_texts(comment)\n",
    "        \n",
    "    seqs = []\n",
    "    for comment in train.Comments:\n",
    "        seqs.append(tokenizer.texts_to_sequences(comment))\n",
    "    \n",
    "    train[\"Comment_Sequences\"] = seqs\n",
    "    \n",
    "    X_train, y_train = split_sequences(train)\n",
    "    \n",
    "    X_train = pad_sequences(X_train, padding=\"post\", truncating=\"post\", maxlen=max_len)\n",
    "    \n",
    "    seqs = []\n",
    "    for comment in test.Comments:\n",
    "        seqs.append(tokenizer.texts_to_sequences(comment))\n",
    "    \n",
    "    test[\"Comment_Sequences\"] = seqs\n",
    "    \n",
    "    X_test, y_test = split_sequences(test)\n",
    "    \n",
    "    X_test = pad_sequences(X_test, padding=\"post\", truncating=\"post\", maxlen=max_len)\n",
    "    \n",
    "    return np.array(X_train.tolist()), np.array(X_test.tolist()), np.array(y_train), np.array(y_test), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acd0d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "955b2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zdude\\AppData\\Local\\Temp\\ipykernel_21880\\2413103618.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"Comment_Sequences\"] = seqs\n",
      "C:\\Users\\zdude\\AppData\\Local\\Temp\\ipykernel_21880\\2413103618.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"Comment_Sequences\"] = seqs\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, tokenizer = prepare_text(twtr, max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee11468",
   "metadata": {},
   "source": [
    "### Creating TensorFlow Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1deeed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 16, input_length=max_len))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(48, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f08b7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           121440    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 48)                1200      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 49        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,097\n",
      "Trainable params: 123,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6811f657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000178764748B0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000178764748B0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017876E7E280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017876E7E280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "236/236 - 1s - loss: 0.6927 - accuracy: 0.5159 - val_loss: 0.7011 - val_accuracy: 0.4293 - 1s/epoch - 5ms/step\n",
      "Epoch 2/100\n",
      "236/236 - 0s - loss: 0.6922 - accuracy: 0.5157 - val_loss: 0.7068 - val_accuracy: 0.4282 - 426ms/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "236/236 - 0s - loss: 0.6875 - accuracy: 0.5439 - val_loss: 0.6874 - val_accuracy: 0.5404 - 390ms/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "236/236 - 0s - loss: 0.6477 - accuracy: 0.6272 - val_loss: 0.7548 - val_accuracy: 0.4725 - 404ms/epoch - 2ms/step\n",
      "Epoch 5/100\n",
      "236/236 - 0s - loss: 0.5645 - accuracy: 0.7037 - val_loss: 0.9188 - val_accuracy: 0.4624 - 420ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "236/236 - 0s - loss: 0.5067 - accuracy: 0.7387 - val_loss: 0.8330 - val_accuracy: 0.5275 - 390ms/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "236/236 - 0s - loss: 0.4706 - accuracy: 0.7649 - val_loss: 0.8820 - val_accuracy: 0.5039 - 407ms/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "236/236 - 0s - loss: 0.4206 - accuracy: 0.7997 - val_loss: 1.0327 - val_accuracy: 0.4989 - 395ms/epoch - 2ms/step\n",
      "Epoch 9/100\n",
      "236/236 - 0s - loss: 0.3918 - accuracy: 0.8095 - val_loss: 1.0665 - val_accuracy: 0.4921 - 401ms/epoch - 2ms/step\n",
      "Epoch 10/100\n",
      "236/236 - 0s - loss: 0.3757 - accuracy: 0.8184 - val_loss: 1.0565 - val_accuracy: 0.5045 - 410ms/epoch - 2ms/step\n",
      "Epoch 11/100\n",
      "236/236 - 0s - loss: 0.3505 - accuracy: 0.8284 - val_loss: 1.1373 - val_accuracy: 0.5006 - 425ms/epoch - 2ms/step\n",
      "Epoch 12/100\n",
      "236/236 - 1s - loss: 0.3402 - accuracy: 0.8346 - val_loss: 1.1636 - val_accuracy: 0.5000 - 511ms/epoch - 2ms/step\n",
      "Epoch 13/100\n",
      "236/236 - 1s - loss: 0.3261 - accuracy: 0.8378 - val_loss: 1.2110 - val_accuracy: 0.4989 - 526ms/epoch - 2ms/step\n",
      "Epoch 14/100\n",
      "236/236 - 0s - loss: 0.3180 - accuracy: 0.8477 - val_loss: 1.2783 - val_accuracy: 0.4949 - 417ms/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "236/236 - 0s - loss: 0.3001 - accuracy: 0.8481 - val_loss: 1.3283 - val_accuracy: 0.4989 - 442ms/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "236/236 - 1s - loss: 0.2899 - accuracy: 0.8548 - val_loss: 1.3316 - val_accuracy: 0.5028 - 550ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "236/236 - 0s - loss: 0.2819 - accuracy: 0.8617 - val_loss: 1.4152 - val_accuracy: 0.4944 - 495ms/epoch - 2ms/step\n",
      "Epoch 18/100\n",
      "236/236 - 0s - loss: 0.2753 - accuracy: 0.8623 - val_loss: 1.4413 - val_accuracy: 0.5011 - 487ms/epoch - 2ms/step\n",
      "Epoch 19/100\n",
      "236/236 - 0s - loss: 0.2738 - accuracy: 0.8621 - val_loss: 1.4406 - val_accuracy: 0.5112 - 474ms/epoch - 2ms/step\n",
      "Epoch 20/100\n",
      "236/236 - 0s - loss: 0.2784 - accuracy: 0.8598 - val_loss: 1.5460 - val_accuracy: 0.4933 - 397ms/epoch - 2ms/step\n",
      "Epoch 21/100\n",
      "236/236 - 0s - loss: 0.2686 - accuracy: 0.8629 - val_loss: 1.4950 - val_accuracy: 0.5039 - 415ms/epoch - 2ms/step\n",
      "Epoch 22/100\n",
      "236/236 - 0s - loss: 0.2567 - accuracy: 0.8687 - val_loss: 1.6051 - val_accuracy: 0.4966 - 419ms/epoch - 2ms/step\n",
      "Epoch 23/100\n",
      "236/236 - 0s - loss: 0.2603 - accuracy: 0.8690 - val_loss: 1.5562 - val_accuracy: 0.5056 - 490ms/epoch - 2ms/step\n",
      "Epoch 24/100\n",
      "236/236 - 0s - loss: 0.2519 - accuracy: 0.8690 - val_loss: 1.5947 - val_accuracy: 0.5056 - 416ms/epoch - 2ms/step\n",
      "Epoch 25/100\n",
      "236/236 - 0s - loss: 0.2467 - accuracy: 0.8771 - val_loss: 1.6811 - val_accuracy: 0.4955 - 407ms/epoch - 2ms/step\n",
      "Epoch 26/100\n",
      "236/236 - 0s - loss: 0.2524 - accuracy: 0.8703 - val_loss: 1.6619 - val_accuracy: 0.5090 - 418ms/epoch - 2ms/step\n",
      "Epoch 27/100\n",
      "236/236 - 0s - loss: 0.2475 - accuracy: 0.8768 - val_loss: 1.7019 - val_accuracy: 0.5056 - 406ms/epoch - 2ms/step\n",
      "Epoch 28/100\n",
      "236/236 - 0s - loss: 0.2405 - accuracy: 0.8771 - val_loss: 1.7340 - val_accuracy: 0.5022 - 422ms/epoch - 2ms/step\n",
      "Epoch 29/100\n",
      "236/236 - 0s - loss: 0.2382 - accuracy: 0.8764 - val_loss: 1.7650 - val_accuracy: 0.4983 - 452ms/epoch - 2ms/step\n",
      "Epoch 30/100\n",
      "236/236 - 1s - loss: 0.2324 - accuracy: 0.8799 - val_loss: 1.7778 - val_accuracy: 0.5090 - 515ms/epoch - 2ms/step\n",
      "Epoch 31/100\n",
      "236/236 - 0s - loss: 0.2581 - accuracy: 0.8673 - val_loss: 1.7635 - val_accuracy: 0.5006 - 414ms/epoch - 2ms/step\n",
      "Epoch 32/100\n",
      "236/236 - 1s - loss: 0.2393 - accuracy: 0.8736 - val_loss: 1.8072 - val_accuracy: 0.4944 - 515ms/epoch - 2ms/step\n",
      "Epoch 33/100\n",
      "236/236 - 0s - loss: 0.2315 - accuracy: 0.8797 - val_loss: 1.8289 - val_accuracy: 0.4961 - 428ms/epoch - 2ms/step\n",
      "Epoch 34/100\n",
      "236/236 - 0s - loss: 0.2336 - accuracy: 0.8781 - val_loss: 1.8165 - val_accuracy: 0.5090 - 384ms/epoch - 2ms/step\n",
      "Epoch 35/100\n",
      "236/236 - 0s - loss: 0.2309 - accuracy: 0.8776 - val_loss: 1.8814 - val_accuracy: 0.4966 - 404ms/epoch - 2ms/step\n",
      "Epoch 36/100\n",
      "236/236 - 0s - loss: 0.2278 - accuracy: 0.8820 - val_loss: 1.9578 - val_accuracy: 0.4933 - 394ms/epoch - 2ms/step\n",
      "Epoch 37/100\n",
      "236/236 - 0s - loss: 0.2243 - accuracy: 0.8804 - val_loss: 1.9030 - val_accuracy: 0.5123 - 385ms/epoch - 2ms/step\n",
      "Epoch 38/100\n",
      "236/236 - 0s - loss: 0.2347 - accuracy: 0.8785 - val_loss: 1.9660 - val_accuracy: 0.4961 - 406ms/epoch - 2ms/step\n",
      "Epoch 39/100\n",
      "236/236 - 0s - loss: 0.2321 - accuracy: 0.8799 - val_loss: 1.9688 - val_accuracy: 0.4927 - 395ms/epoch - 2ms/step\n",
      "Epoch 40/100\n",
      "236/236 - 0s - loss: 0.2223 - accuracy: 0.8842 - val_loss: 1.9648 - val_accuracy: 0.5112 - 393ms/epoch - 2ms/step\n",
      "Epoch 41/100\n",
      "236/236 - 0s - loss: 0.2305 - accuracy: 0.8800 - val_loss: 1.9330 - val_accuracy: 0.5101 - 388ms/epoch - 2ms/step\n",
      "Epoch 42/100\n",
      "236/236 - 0s - loss: 0.2414 - accuracy: 0.8728 - val_loss: 1.9080 - val_accuracy: 0.5056 - 373ms/epoch - 2ms/step\n",
      "Epoch 43/100\n",
      "236/236 - 0s - loss: 0.2274 - accuracy: 0.8813 - val_loss: 1.9360 - val_accuracy: 0.4994 - 379ms/epoch - 2ms/step\n",
      "Epoch 44/100\n",
      "236/236 - 0s - loss: 0.2186 - accuracy: 0.8882 - val_loss: 2.0379 - val_accuracy: 0.4944 - 396ms/epoch - 2ms/step\n",
      "Epoch 45/100\n",
      "236/236 - 0s - loss: 0.2228 - accuracy: 0.8842 - val_loss: 1.9841 - val_accuracy: 0.5056 - 383ms/epoch - 2ms/step\n",
      "Epoch 46/100\n",
      "236/236 - 0s - loss: 0.2191 - accuracy: 0.8852 - val_loss: 2.0136 - val_accuracy: 0.4966 - 390ms/epoch - 2ms/step\n",
      "Epoch 47/100\n",
      "236/236 - 0s - loss: 0.2162 - accuracy: 0.8890 - val_loss: 2.2245 - val_accuracy: 0.4877 - 407ms/epoch - 2ms/step\n",
      "Epoch 48/100\n",
      "236/236 - 0s - loss: 0.2281 - accuracy: 0.8815 - val_loss: 2.0006 - val_accuracy: 0.5056 - 383ms/epoch - 2ms/step\n",
      "Epoch 49/100\n",
      "236/236 - 0s - loss: 0.2232 - accuracy: 0.8808 - val_loss: 1.9944 - val_accuracy: 0.5112 - 388ms/epoch - 2ms/step\n",
      "Epoch 50/100\n",
      "236/236 - 0s - loss: 0.2164 - accuracy: 0.8857 - val_loss: 2.0946 - val_accuracy: 0.4916 - 399ms/epoch - 2ms/step\n",
      "Epoch 51/100\n",
      "236/236 - 0s - loss: 0.2188 - accuracy: 0.8862 - val_loss: 2.0657 - val_accuracy: 0.5039 - 400ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "236/236 - 0s - loss: 0.2213 - accuracy: 0.8854 - val_loss: 2.1209 - val_accuracy: 0.4944 - 390ms/epoch - 2ms/step\n",
      "Epoch 53/100\n",
      "236/236 - 0s - loss: 0.2198 - accuracy: 0.8848 - val_loss: 2.1093 - val_accuracy: 0.4955 - 391ms/epoch - 2ms/step\n",
      "Epoch 54/100\n",
      "236/236 - 0s - loss: 0.2133 - accuracy: 0.8888 - val_loss: 2.0832 - val_accuracy: 0.5101 - 406ms/epoch - 2ms/step\n",
      "Epoch 55/100\n",
      "236/236 - 0s - loss: 0.2218 - accuracy: 0.8812 - val_loss: 2.1402 - val_accuracy: 0.4972 - 396ms/epoch - 2ms/step\n",
      "Epoch 56/100\n",
      "236/236 - 0s - loss: 0.2164 - accuracy: 0.8860 - val_loss: 2.1094 - val_accuracy: 0.5039 - 387ms/epoch - 2ms/step\n",
      "Epoch 57/100\n",
      "236/236 - 0s - loss: 0.2177 - accuracy: 0.8858 - val_loss: 2.1194 - val_accuracy: 0.5034 - 376ms/epoch - 2ms/step\n",
      "Epoch 58/100\n",
      "236/236 - 0s - loss: 0.2121 - accuracy: 0.8898 - val_loss: 2.1694 - val_accuracy: 0.4921 - 376ms/epoch - 2ms/step\n",
      "Epoch 59/100\n",
      "236/236 - 0s - loss: 0.2159 - accuracy: 0.8853 - val_loss: 2.1987 - val_accuracy: 0.4933 - 411ms/epoch - 2ms/step\n",
      "Epoch 60/100\n",
      "236/236 - 0s - loss: 0.2104 - accuracy: 0.8889 - val_loss: 2.1810 - val_accuracy: 0.5000 - 382ms/epoch - 2ms/step\n",
      "Epoch 61/100\n",
      "236/236 - 0s - loss: 0.2062 - accuracy: 0.8896 - val_loss: 2.3462 - val_accuracy: 0.4837 - 460ms/epoch - 2ms/step\n",
      "Epoch 62/100\n",
      "236/236 - 1s - loss: 0.2120 - accuracy: 0.8877 - val_loss: 2.4030 - val_accuracy: 0.4832 - 559ms/epoch - 2ms/step\n",
      "Epoch 63/100\n",
      "236/236 - 0s - loss: 0.2170 - accuracy: 0.8869 - val_loss: 2.2286 - val_accuracy: 0.5017 - 386ms/epoch - 2ms/step\n",
      "Epoch 64/100\n",
      "236/236 - 0s - loss: 0.2093 - accuracy: 0.8893 - val_loss: 2.2587 - val_accuracy: 0.5000 - 394ms/epoch - 2ms/step\n",
      "Epoch 65/100\n",
      "236/236 - 0s - loss: 0.2143 - accuracy: 0.8882 - val_loss: 2.3158 - val_accuracy: 0.4865 - 397ms/epoch - 2ms/step\n",
      "Epoch 66/100\n",
      "236/236 - 0s - loss: 0.2138 - accuracy: 0.8862 - val_loss: 2.2101 - val_accuracy: 0.4983 - 401ms/epoch - 2ms/step\n",
      "Epoch 67/100\n",
      "236/236 - 0s - loss: 0.2048 - accuracy: 0.8913 - val_loss: 2.4076 - val_accuracy: 0.4815 - 415ms/epoch - 2ms/step\n",
      "Epoch 68/100\n",
      "236/236 - 0s - loss: 0.2091 - accuracy: 0.8925 - val_loss: 2.3071 - val_accuracy: 0.4994 - 396ms/epoch - 2ms/step\n",
      "Epoch 69/100\n",
      "236/236 - 0s - loss: 0.2067 - accuracy: 0.8894 - val_loss: 2.3442 - val_accuracy: 0.4944 - 393ms/epoch - 2ms/step\n",
      "Epoch 70/100\n",
      "236/236 - 0s - loss: 0.2039 - accuracy: 0.8930 - val_loss: 2.3524 - val_accuracy: 0.5028 - 399ms/epoch - 2ms/step\n",
      "Epoch 71/100\n",
      "236/236 - 0s - loss: 0.2122 - accuracy: 0.8893 - val_loss: 2.2901 - val_accuracy: 0.5011 - 375ms/epoch - 2ms/step\n",
      "Epoch 72/100\n",
      "236/236 - 0s - loss: 0.2137 - accuracy: 0.8886 - val_loss: 2.2813 - val_accuracy: 0.5028 - 403ms/epoch - 2ms/step\n",
      "Epoch 73/100\n",
      "236/236 - 0s - loss: 0.2065 - accuracy: 0.8906 - val_loss: 2.3087 - val_accuracy: 0.5028 - 378ms/epoch - 2ms/step\n",
      "Epoch 74/100\n",
      "236/236 - 0s - loss: 0.2047 - accuracy: 0.8882 - val_loss: 2.3963 - val_accuracy: 0.4961 - 395ms/epoch - 2ms/step\n",
      "Epoch 75/100\n",
      "236/236 - 0s - loss: 0.2119 - accuracy: 0.8848 - val_loss: 2.3134 - val_accuracy: 0.5051 - 383ms/epoch - 2ms/step\n",
      "Epoch 76/100\n",
      "236/236 - 0s - loss: 0.2031 - accuracy: 0.8885 - val_loss: 2.3918 - val_accuracy: 0.4989 - 394ms/epoch - 2ms/step\n",
      "Epoch 77/100\n",
      "236/236 - 0s - loss: 0.2133 - accuracy: 0.8878 - val_loss: 2.3565 - val_accuracy: 0.4966 - 492ms/epoch - 2ms/step\n",
      "Epoch 78/100\n",
      "236/236 - 0s - loss: 0.2103 - accuracy: 0.8922 - val_loss: 2.3039 - val_accuracy: 0.5011 - 389ms/epoch - 2ms/step\n",
      "Epoch 79/100\n",
      "236/236 - 0s - loss: 0.2040 - accuracy: 0.8919 - val_loss: 2.4415 - val_accuracy: 0.4927 - 385ms/epoch - 2ms/step\n",
      "Epoch 80/100\n",
      "236/236 - 0s - loss: 0.2054 - accuracy: 0.8914 - val_loss: 2.3822 - val_accuracy: 0.4978 - 404ms/epoch - 2ms/step\n",
      "Epoch 81/100\n",
      "236/236 - 0s - loss: 0.2133 - accuracy: 0.8872 - val_loss: 2.3459 - val_accuracy: 0.5006 - 412ms/epoch - 2ms/step\n",
      "Epoch 82/100\n",
      "236/236 - 0s - loss: 0.2043 - accuracy: 0.8897 - val_loss: 2.4031 - val_accuracy: 0.4938 - 387ms/epoch - 2ms/step\n",
      "Epoch 83/100\n",
      "236/236 - 0s - loss: 0.2007 - accuracy: 0.8922 - val_loss: 2.4100 - val_accuracy: 0.4972 - 401ms/epoch - 2ms/step\n",
      "Epoch 84/100\n",
      "236/236 - 0s - loss: 0.2003 - accuracy: 0.8942 - val_loss: 2.5081 - val_accuracy: 0.4955 - 407ms/epoch - 2ms/step\n",
      "Epoch 85/100\n",
      "236/236 - 0s - loss: 0.1995 - accuracy: 0.8923 - val_loss: 2.5425 - val_accuracy: 0.4933 - 402ms/epoch - 2ms/step\n",
      "Epoch 86/100\n",
      "236/236 - 0s - loss: 0.1992 - accuracy: 0.8930 - val_loss: 2.4895 - val_accuracy: 0.4972 - 397ms/epoch - 2ms/step\n",
      "Epoch 87/100\n",
      "236/236 - 0s - loss: 0.2009 - accuracy: 0.8917 - val_loss: 2.5413 - val_accuracy: 0.4927 - 385ms/epoch - 2ms/step\n",
      "Epoch 88/100\n",
      "236/236 - 0s - loss: 0.2034 - accuracy: 0.8892 - val_loss: 2.4776 - val_accuracy: 0.5017 - 478ms/epoch - 2ms/step\n",
      "Epoch 89/100\n",
      "236/236 - 0s - loss: 0.2115 - accuracy: 0.8892 - val_loss: 2.4529 - val_accuracy: 0.4994 - 449ms/epoch - 2ms/step\n",
      "Epoch 90/100\n",
      "236/236 - 0s - loss: 0.2003 - accuracy: 0.8926 - val_loss: 2.4603 - val_accuracy: 0.4989 - 424ms/epoch - 2ms/step\n",
      "Epoch 91/100\n",
      "236/236 - 0s - loss: 0.2032 - accuracy: 0.8930 - val_loss: 2.4557 - val_accuracy: 0.4989 - 448ms/epoch - 2ms/step\n",
      "Epoch 92/100\n",
      "236/236 - 0s - loss: 0.1988 - accuracy: 0.8962 - val_loss: 2.5135 - val_accuracy: 0.5006 - 462ms/epoch - 2ms/step\n",
      "Epoch 93/100\n",
      "236/236 - 0s - loss: 0.2048 - accuracy: 0.8906 - val_loss: 2.4620 - val_accuracy: 0.4972 - 478ms/epoch - 2ms/step\n",
      "Epoch 94/100\n",
      "236/236 - 0s - loss: 0.1993 - accuracy: 0.8939 - val_loss: 2.6071 - val_accuracy: 0.4933 - 436ms/epoch - 2ms/step\n",
      "Epoch 95/100\n",
      "236/236 - 0s - loss: 0.2000 - accuracy: 0.8917 - val_loss: 2.5497 - val_accuracy: 0.4994 - 447ms/epoch - 2ms/step\n",
      "Epoch 96/100\n",
      "236/236 - 0s - loss: 0.1997 - accuracy: 0.8946 - val_loss: 2.5490 - val_accuracy: 0.5006 - 407ms/epoch - 2ms/step\n",
      "Epoch 97/100\n",
      "236/236 - 0s - loss: 0.2016 - accuracy: 0.8929 - val_loss: 2.5290 - val_accuracy: 0.4994 - 430ms/epoch - 2ms/step\n",
      "Epoch 98/100\n",
      "236/236 - 0s - loss: 0.2022 - accuracy: 0.8925 - val_loss: 2.5353 - val_accuracy: 0.4966 - 414ms/epoch - 2ms/step\n",
      "Epoch 99/100\n",
      "236/236 - 0s - loss: 0.1999 - accuracy: 0.8942 - val_loss: 2.5033 - val_accuracy: 0.5051 - 442ms/epoch - 2ms/step\n",
      "Epoch 100/100\n",
      "236/236 - 0s - loss: 0.1976 - accuracy: 0.8930 - val_loss: 2.5970 - val_accuracy: 0.4955 - 432ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x178772c8fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000017876E648B0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000017876E648B0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf6c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred = []\n",
    "for i in pred:\n",
    "    real_pred.append(1 if i > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75acf022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b1a1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49551066217732886"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(real_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aee8df",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4516939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, model, train_proportion=0.8):\n",
    "    days = []\n",
    "    actual = []\n",
    "    pred_proportion = []\n",
    "    pred_val = []\n",
    "    \n",
    "    for date in data.Date.unique():\n",
    "        days.append(date)\n",
    "        actual.append()\n",
    "        day = data[data[\"Date\"] == date]\n",
    "        predictions = []\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
