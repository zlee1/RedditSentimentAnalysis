{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fitting-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies\n",
    "#!pip install praw\n",
    "#!pip install psaw\n",
    "#!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nominated-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Suppress warning messages\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "refined-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load client_id, secret_id, and user_agent\n",
    "with open('info.json') as f:\n",
    "     info = json.load(f)\n",
    "        \n",
    "info = dict(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laughing-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit and PushshiftAPI instances\n",
    "reddit = praw.Reddit(client_id=info[\"client_id\"], user_agent=info[\"user_agent\"], client_secret=info[\"client_secret\"])\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70e2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results of a search in a DataFrame\n",
    "\"\"\"\n",
    "subm_dicts = [{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in api.search_submissions(subreddit='stocks', q=\"TWTR\", filter=['url','author', 'title', 'subreddit'], limit=100)]\n",
    "df = pd.DataFrame(subm_dicts)\n",
    "df\n",
    "\"\"\"\n",
    "_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5295a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-11-07</th>\n",
       "      <td>45.099998</td>\n",
       "      <td>50.090000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>44.900002</td>\n",
       "      <td>117701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-08</th>\n",
       "      <td>45.930000</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>40.689999</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>41.650002</td>\n",
       "      <td>27925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-11</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>39.400002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>42.900002</td>\n",
       "      <td>16113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-12</th>\n",
       "      <td>43.660000</td>\n",
       "      <td>43.779999</td>\n",
       "      <td>41.830002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>6316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-13</th>\n",
       "      <td>41.029999</td>\n",
       "      <td>42.869999</td>\n",
       "      <td>40.759998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>8688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-04</th>\n",
       "      <td>47.869999</td>\n",
       "      <td>51.369999</td>\n",
       "      <td>46.860001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>268465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>53.849998</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>50.560001</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>50.980000</td>\n",
       "      <td>217520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-06</th>\n",
       "      <td>50.040001</td>\n",
       "      <td>52.869999</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>159034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-07</th>\n",
       "      <td>50.470001</td>\n",
       "      <td>51.639999</td>\n",
       "      <td>46.549999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>48.029999</td>\n",
       "      <td>120844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-08</th>\n",
       "      <td>47.299999</td>\n",
       "      <td>48.439999</td>\n",
       "      <td>45.830002</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>83262600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2013-11-07  45.099998  50.090000  44.000000  44.900002  44.900002  117701600\n",
       "2013-11-08  45.930000  46.939999  40.689999  41.650002  41.650002   27925300\n",
       "2013-11-11  40.500000  43.000000  39.400002  42.900002  42.900002   16113900\n",
       "2013-11-12  43.660000  43.779999  41.830002  41.900002  41.900002    6316700\n",
       "2013-11-13  41.029999  42.869999  40.759998  42.599998  42.599998    8688300\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2022-04-04  47.869999  51.369999  46.860001  49.970001  49.970001  268465400\n",
       "2022-04-05  53.849998  54.570000  50.560001  50.980000  50.980000  217520100\n",
       "2022-04-06  50.040001  52.869999  49.299999  50.770000  50.770000  159034700\n",
       "2022-04-07  50.470001  51.639999  46.549999  48.029999  48.029999  120844100\n",
       "2022-04-08  47.299999  48.439999  45.830002  46.230000  46.230000   83262600\n",
       "\n",
       "[2120 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get historical stock data for a ticker\n",
    "#twtr = yf.download('TWTR', progress=True)\n",
    "#twtr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cee07",
   "metadata": {},
   "source": [
    "## Processing Tickers\n",
    "\n",
    "This initial thought process is not great. I decided that looking for specific words wwould not be a good idea, as it takes much of the context out of the comment. (Something that briefly mentions TSLA, but is actually talking about how great MSFT is would be useless in predicting TSLA stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5ce231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At close, calculate the real and percent change since last close\n",
    "def get_diff(ticker_data):\n",
    "    df = ticker_data.copy()\n",
    "    real = []\n",
    "    percent = []\n",
    "    for index, row in df.reset_index().iterrows():\n",
    "        if(index == 0):\n",
    "            real.append(0)\n",
    "            percent.append(0)\n",
    "        else:\n",
    "            real.append(row[\"Close\"]-df.iloc[index-1][\"Close\"])\n",
    "            percent.append(real[-1]/df.iloc[index-1][\"Close\"])\n",
    "    return real, percent\n",
    "\n",
    "# Get the reddit posts that mention a certain ticker n days before a large change in stock price\n",
    "def get_pre_change_posts(ticker, ticker_gain, days=1, limit=1000, subreddit=\"stocks,stockmarket,stocksandtrading,daytrading,investing,stocks_picks,stockstobuytoday\"):\n",
    "    df = None\n",
    "    for index, row in ticker_gain.iterrows():\n",
    "        start_date = datetime.fromtimestamp(row[\"Date\"].timestamp()) + timedelta(hours=6, days=-days)\n",
    "        end_date = datetime.fromtimestamp(row[\"Date\"].timestamp()) + timedelta(hours=6)\n",
    "        \n",
    "        # TODO: Check whether comments would be better than submissions\n",
    "        \n",
    "        submissions = api.search_comments(after=start_date, before=end_date, q=ticker, subreddit=subreddit, filter=['url','author', 'title', 'subreddit'], limit=limit)\n",
    "        if(df is None):\n",
    "            df = pd.DataFrame([{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in submissions])\n",
    "        else:\n",
    "            df = df.append([{k:getattr(praw_obj, k) for k in vars(praw_obj)} for praw_obj in submissions], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def word_counts(df, column=\"body\", min_letters=3):\n",
    "    counts = {}\n",
    "    for i in list(df[column]):\n",
    "        for j in i.split(\" \"):\n",
    "            j = ''.join(k for k in j if k.isalnum())\n",
    "            # Exclude words that are likely tickers\n",
    "            if(j == j.upper() and len(j) > 1 and len(j) <= 5):\n",
    "                pass\n",
    "            elif(len(j) < 3):\n",
    "                pass\n",
    "            elif(j not in counts.keys()):\n",
    "                counts[j.lower()] = 1\n",
    "            else:\n",
    "                counts.update({j.lower():counts.get(j.lower())+1})\n",
    "    return counts\n",
    "\n",
    "def remove_shared_keys(dict_a, dict_b, cutoff=2):\n",
    "    a = dict_a.copy()\n",
    "    b = dict_b.copy()\n",
    "    \n",
    "    rm_a = []\n",
    "    rm_b = []\n",
    "    for i in a:\n",
    "        if(i in b):\n",
    "            if(b.get(i) > 2*a.get(i)):\n",
    "                rm_a.append(i)\n",
    "            elif(b.get(i) < 2*a.get(i)):\n",
    "                rm_b.append(i)\n",
    "            else:\n",
    "                rm_a.append(i)\n",
    "                rm_b.append(i)\n",
    "    for i in rm_a:\n",
    "        a.pop(i)\n",
    "    for i in rm_b:\n",
    "        b.pop(i)\n",
    "    return a, b\n",
    "\n",
    "def remove_infrequent_words(dict_a, min_count=2):\n",
    "    d = dict_a.copy()\n",
    "    \n",
    "    to_remove = []\n",
    "    for i, x in d.items():\n",
    "        if(x < min_count):\n",
    "            to_remove.append(i)\n",
    "            \n",
    "    for i in to_remove:\n",
    "        d.pop(i)\n",
    "        \n",
    "    return d\n",
    "\n",
    "# Generate information for a given ticker\n",
    "def process_ticker(ticker, gain_cutoff=0.05, loss_cutoff=0.05, limit=100, days=1):\n",
    "    try:\n",
    "        ticker_data = yf.download(ticker, progress=False)\n",
    "        ticker_data.reset_index(inplace=True)\n",
    "        real, percent = get_diff(ticker_data)\n",
    "\n",
    "        ticker_data[\"Real_Change\"] = real\n",
    "        ticker_data[\"Percent_Change\"] = percent\n",
    "\n",
    "        ticker_gain = ticker_data[ticker_data[\"Percent_Change\"] > gain_cutoff]\n",
    "        ticker_loss = ticker_data[ticker_data[\"Percent_Change\"] < -loss_cutoff]\n",
    "\n",
    "        pre_gain = get_pre_change_posts(ticker, ticker_gain, days, limit)\n",
    "        pre_loss = get_pre_change_posts(ticker, ticker_loss, days, limit)\n",
    "\n",
    "        gain_wc = dict(sorted(word_counts(pre_gain).items(), key=lambda x: x[1], reverse=True))\n",
    "        loss_wc = dict(sorted(word_counts(pre_loss).items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        gain_freq = remove_infrequent_words(gain_wc)\n",
    "        loss_freq = remove_infrequent_words(loss_wc)\n",
    "\n",
    "        gain_only, loss_only = remove_shared_keys(gain_freq, loss_freq)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}, {}\n",
    "    \n",
    "    return gain_only, loss_only\n",
    "\n",
    "#gain_only, loss_only = process_ticker(\"FB\")\n",
    "\n",
    "#gain_only\n",
    "\n",
    "#loss_only\n",
    "\"\"\"\n",
    "all_gain = []\n",
    "all_loss = []\n",
    "for ticker in [\"TWTR\", \"FB\", \"MSFT\", \"ADBE\", \"AAPL\", \"SNAP\", \"AMZN\", \"NCL\", \"DIS\", \"NFLX\"]:\n",
    "    gain_only, loss_only = process_ticker(ticker)\n",
    "    all_gain.append(gain_only)\n",
    "    all_loss.append(loss_only)\n",
    "    print(ticker)\n",
    "\n",
    "def combine_dict_list(list_of_dicts):\n",
    "    single_dict = {}\n",
    "    for d in list_of_dicts:\n",
    "        for i in d:\n",
    "            if(i not in single_dict):\n",
    "                single_dict[i] = d.get(i)\n",
    "            else:\n",
    "                single_dict.update({i:single_dict.get(i)+d.get(i)})\n",
    "    return dict(sorted(single_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "gain, loss = remove_shared_keys(combine_dict_list(all_gain), combine_dict_list(all_loss))\n",
    "\n",
    "gain\n",
    "\n",
    "loss\"\"\"\n",
    "_ = None # This is just to stop automatic output of block commented code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353ac1d",
   "metadata": {},
   "source": [
    "## Better Method (Probably)\n",
    "\n",
    "Instead of looking at posts/comments the day before and predicting whether the next day will close higher, this will be looking at the posts/comments from the previous day's close to the current day's open and predicting whether the close price will be higher than the open price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e3f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_change(data):\n",
    "    change = []\n",
    "    up = []\n",
    "    for index, row in data.iterrows():\n",
    "        change.append(row[\"Close\"]-row[\"Open\"])\n",
    "        up.append(int(change[-1] > 0))\n",
    "    return change, up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8123782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change, up = daily_change(twtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de5a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#twtr[\"Daily_Change\"] = change\n",
    "#twtr[\"Positive_Change\"] = up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d819a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#twtr = twtr.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3f798a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Change</th>\n",
       "      <th>Positive_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>35.049999</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>34058000</td>\n",
       "      <td>-0.889999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-27</th>\n",
       "      <td>15.150000</td>\n",
       "      <td>15.280000</td>\n",
       "      <td>14.810000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>84880300</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-22</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.340000</td>\n",
       "      <td>48.110001</td>\n",
       "      <td>50.279999</td>\n",
       "      <td>50.279999</td>\n",
       "      <td>18289000</td>\n",
       "      <td>0.279999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12</th>\n",
       "      <td>70.870003</td>\n",
       "      <td>71.839996</td>\n",
       "      <td>69.650002</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>9495200</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-02</th>\n",
       "      <td>32.889999</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31209800</td>\n",
       "      <td>-1.139999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-30</th>\n",
       "      <td>41.560001</td>\n",
       "      <td>42.480000</td>\n",
       "      <td>39.939999</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>42012600</td>\n",
       "      <td>0.239998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>31.170000</td>\n",
       "      <td>31.190001</td>\n",
       "      <td>30.280001</td>\n",
       "      <td>30.620001</td>\n",
       "      <td>30.620001</td>\n",
       "      <td>12360700</td>\n",
       "      <td>-0.549999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.680000</td>\n",
       "      <td>54.369999</td>\n",
       "      <td>54.910000</td>\n",
       "      <td>54.910000</td>\n",
       "      <td>9259200</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-18</th>\n",
       "      <td>40.689999</td>\n",
       "      <td>41.169998</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>18605000</td>\n",
       "      <td>-0.079998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-23</th>\n",
       "      <td>63.910000</td>\n",
       "      <td>66.769997</td>\n",
       "      <td>63.790001</td>\n",
       "      <td>66.489998</td>\n",
       "      <td>66.489998</td>\n",
       "      <td>18548000</td>\n",
       "      <td>2.579998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2019-02-06  35.049999  35.250000  33.750000  34.160000  34.160000  34058000   \n",
       "2016-04-27  15.150000  15.280000  14.810000  14.860000  14.860000  84880300   \n",
       "2020-10-22  50.000000  50.340000  48.110001  50.279999  50.279999  18289000   \n",
       "2021-04-12  70.870003  71.839996  69.650002  70.860001  70.860001   9495200   \n",
       "2014-06-02  32.889999  33.200001  31.620001  31.750000  31.750000  31209800   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2014-10-30  41.560001  42.480000  39.939999  41.799999  41.799999  42012600   \n",
       "2019-03-01  31.170000  31.190001  30.280001  30.620001  30.620001  12360700   \n",
       "2020-12-22  55.000000  55.680000  54.369999  54.910000  54.910000   9259200   \n",
       "2014-11-18  40.689999  41.169998  39.950001  40.610001  40.610001  18605000   \n",
       "2021-06-23  63.910000  66.769997  63.790001  66.489998  66.489998  18548000   \n",
       "\n",
       "            Daily_Change  Positive_Change  \n",
       "Date                                       \n",
       "2019-02-06     -0.889999                0  \n",
       "2016-04-27     -0.290000                0  \n",
       "2020-10-22      0.279999                1  \n",
       "2021-04-12     -0.010002                0  \n",
       "2014-06-02     -1.139999                0  \n",
       "...                  ...              ...  \n",
       "2014-10-30      0.239998                1  \n",
       "2019-03-01     -0.549999                0  \n",
       "2020-12-22     -0.090000                0  \n",
       "2014-11-18     -0.079998                0  \n",
       "2021-06-23      2.579998                1  \n",
       "\n",
       "[2120 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#twtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82049eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_open_content(data, ticker, start_hour_diff=0, subreddit=\"stocks,stockmarket,stocksandtrading,daytrading,investing,stocks_picks,stockstobuytoday\", limit=100):\n",
    "    new_col = []\n",
    "    for index, row in data.iterrows():\n",
    "        end_time = row.name + timedelta(hours=9, minutes=30)\n",
    "        start_time = end_time - timedelta(hours=17, minutes=30)\n",
    "        content = []\n",
    "        for i in api.search_comments(after=start_time, before=end_time, subreddit=subreddit, q=ticker, filter=['url','author', 'title', 'subreddit'], limit=limit):\n",
    "            for j in i.body.split(\".\"):\n",
    "                for k in j.split(\"\\n\"):\n",
    "                    content.append(k)\n",
    "        new_col.append(content)\n",
    "    \n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03f54ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes quite a while to run. Looking into how to speed it up. \n",
    "#twtr_full = twtr.copy()\n",
    "#twtr = twtr.head(10)\n",
    "#twtr[\"Comments\"] = get_pre_open_content(twtr, \"TWTR\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4435fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Change</th>\n",
       "      <th>Positive_Change</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>35.049999</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>34058000</td>\n",
       "      <td>-0.889999</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-27</th>\n",
       "      <td>15.150000</td>\n",
       "      <td>15.280000</td>\n",
       "      <td>14.810000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>84880300</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>0</td>\n",
       "      <td>[Some people say TWTR is done for, , , Others ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-22</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.340000</td>\n",
       "      <td>48.110001</td>\n",
       "      <td>50.279999</td>\n",
       "      <td>50.279999</td>\n",
       "      <td>18289000</td>\n",
       "      <td>0.279999</td>\n",
       "      <td>1</td>\n",
       "      <td>[FB, TWTR  &amp; MTCH are saving my portfolio all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12</th>\n",
       "      <td>70.870003</td>\n",
       "      <td>71.839996</td>\n",
       "      <td>69.650002</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>9495200</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-02</th>\n",
       "      <td>32.889999</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31209800</td>\n",
       "      <td>-1.139999</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-30</th>\n",
       "      <td>41.560001</td>\n",
       "      <td>42.480000</td>\n",
       "      <td>39.939999</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>42012600</td>\n",
       "      <td>0.239998</td>\n",
       "      <td>1</td>\n",
       "      <td>[I typically do weekly options on volatile sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>31.170000</td>\n",
       "      <td>31.190001</td>\n",
       "      <td>30.280001</td>\n",
       "      <td>30.620001</td>\n",
       "      <td>30.620001</td>\n",
       "      <td>12360700</td>\n",
       "      <td>-0.549999</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.680000</td>\n",
       "      <td>54.369999</td>\n",
       "      <td>54.910000</td>\n",
       "      <td>54.910000</td>\n",
       "      <td>9259200</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-18</th>\n",
       "      <td>40.689999</td>\n",
       "      <td>41.169998</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>40.610001</td>\n",
       "      <td>18605000</td>\n",
       "      <td>-0.079998</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-23</th>\n",
       "      <td>63.910000</td>\n",
       "      <td>66.769997</td>\n",
       "      <td>63.790001</td>\n",
       "      <td>66.489998</td>\n",
       "      <td>66.489998</td>\n",
       "      <td>18548000</td>\n",
       "      <td>2.579998</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2019-02-06  35.049999  35.250000  33.750000  34.160000  34.160000  34058000   \n",
       "2016-04-27  15.150000  15.280000  14.810000  14.860000  14.860000  84880300   \n",
       "2020-10-22  50.000000  50.340000  48.110001  50.279999  50.279999  18289000   \n",
       "2021-04-12  70.870003  71.839996  69.650002  70.860001  70.860001   9495200   \n",
       "2014-06-02  32.889999  33.200001  31.620001  31.750000  31.750000  31209800   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2014-10-30  41.560001  42.480000  39.939999  41.799999  41.799999  42012600   \n",
       "2019-03-01  31.170000  31.190001  30.280001  30.620001  30.620001  12360700   \n",
       "2020-12-22  55.000000  55.680000  54.369999  54.910000  54.910000   9259200   \n",
       "2014-11-18  40.689999  41.169998  39.950001  40.610001  40.610001  18605000   \n",
       "2021-06-23  63.910000  66.769997  63.790001  66.489998  66.489998  18548000   \n",
       "\n",
       "            Daily_Change  Positive_Change  \\\n",
       "Date                                        \n",
       "2019-02-06     -0.889999                0   \n",
       "2016-04-27     -0.290000                0   \n",
       "2020-10-22      0.279999                1   \n",
       "2021-04-12     -0.010002                0   \n",
       "2014-06-02     -1.139999                0   \n",
       "...                  ...              ...   \n",
       "2014-10-30      0.239998                1   \n",
       "2019-03-01     -0.549999                0   \n",
       "2020-12-22     -0.090000                0   \n",
       "2014-11-18     -0.079998                0   \n",
       "2021-06-23      2.579998                1   \n",
       "\n",
       "                                                     Comments  \n",
       "Date                                                           \n",
       "2019-02-06                                                 []  \n",
       "2016-04-27  [Some people say TWTR is done for, , , Others ...  \n",
       "2020-10-22  [FB, TWTR  & MTCH are saving my portfolio all ...  \n",
       "2021-04-12                                                 []  \n",
       "2014-06-02                                                 []  \n",
       "...                                                       ...  \n",
       "2014-10-30  [I typically do weekly options on volatile sto...  \n",
       "2019-03-01                                                 []  \n",
       "2020-12-22                                                 []  \n",
       "2014-11-18                                                 []  \n",
       "2021-06-23                                                 []  \n",
       "\n",
       "[2120 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#twtr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d761d1c",
   "metadata": {},
   "source": [
    "### Preparing text for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(data):\n",
    "    dates = []\n",
    "    seqs = []\n",
    "    vals = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        for comment in row[\"Comment_Sequences\"]:\n",
    "            if(comment != []):\n",
    "                dates.append(row.name)\n",
    "                seqs.append(list(comment))\n",
    "                vals.append(row[\"Positive_Change\"])\n",
    "    return pd.DataFrame({\"Date\":dates, \"Sequence\":seqs, \"Positive_Change\":vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b7e456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(data, train_proportion = 0.8, max_len=50, tokenizer=None):\n",
    "    if(tokenizer is None):\n",
    "        tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "    \n",
    "    train = data[:int(data.shape[0]*train_proportion)]\n",
    "    test = data[int(data.shape[0]*train_proportion):]\n",
    "    \n",
    "    for comment in train.Comments:\n",
    "        tokenizer.fit_on_texts(comment)\n",
    "        \n",
    "    seqs = []\n",
    "    for comment in train.Comments:\n",
    "        seqs.append(tokenizer.texts_to_sequences(comment))\n",
    "    \n",
    "    train[\"Comment_Sequences\"] = seqs\n",
    "    \n",
    "    train = split_sequences(train)\n",
    "        \n",
    "    train_padded = pad_sequences(train[\"Sequence\"], padding=\"post\", truncating=\"post\", maxlen=max_len)\n",
    "\n",
    "    for comment in test.Comments:\n",
    "        tokenizer.fit_on_texts(comment)\n",
    "        \n",
    "    seqs = []\n",
    "    for comment in test.Comments:\n",
    "        seqs.append(tokenizer.texts_to_sequences(comment))\n",
    "    \n",
    "    test[\"Comment_Sequences\"] = seqs\n",
    "    \n",
    "    test = split_sequences(test)\n",
    "        \n",
    "    test_padded = pad_sequences(test[\"Sequence\"], padding=\"post\", truncating=\"post\", maxlen=max_len)\n",
    "    \n",
    "    return train, train_padded, test, test_padded, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b9654423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ticker_data(tickers = None):\n",
    "    if(tickers is None): return None\n",
    "    \n",
    "    cur_tokenizer = None\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        if(f\"{ticker}.pkl\" in os.listdir('data/')):\n",
    "            continue\n",
    "        \n",
    "        data = yf.download(ticker)\n",
    "        change, up = daily_change(data)\n",
    "        data[\"Daily_Change\"] = change\n",
    "        data[\"Positive_Change\"] = up\n",
    "        data = data.sample(frac=1)\n",
    "        \n",
    "        data[\"Comments\"] = get_pre_open_content(data, ticker, limit=100)\n",
    "        \n",
    "        train, train_padded, test, test_padded, cur_tokenizer = prepare_text(data, tokenizer=cur_tokenizer)\n",
    "        \n",
    "        with open(f\"data/{ticker}.pkl\", \"wb+\") as f:\n",
    "            pickle.dump((train, train_padded, test, test_padded), f)\n",
    "            \n",
    "        used_tickers = \"_\".join(tickers[:tickers.index(ticker)+1])\n",
    "        \n",
    "        with open(f\"data/tokenizer/tokenizer_{used_tickers}.pkl\", \"wb+\") as f:\n",
    "            pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ef209f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zdude\\Anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "C:\\Users\\zdude\\Anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\zdude\\Anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "C:\\Users\\zdude\\AppData\\Local\\Temp\\ipykernel_10588\\3286485899.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"Comment_Sequences\"] = seqs\n",
      "C:\\Users\\zdude\\AppData\\Local\\Temp\\ipykernel_10588\\3286485899.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"Comment_Sequences\"] = seqs\n"
     ]
    }
   ],
   "source": [
    "generate_ticker_data([\"TWTR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "acd0d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"data/TWTR.pkl\", \"rb\") as f:\n",
    "#    train, train_padded, test, test_padded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "10dbf757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   0,   0, ...,   0,   0,   0],\n",
       "       [  7,   0,   0, ...,   0,   0,   0],\n",
       "       [194,  51,  23, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  6,  60,  28, ...,   0,   0,   0],\n",
       "       [104, 463,  14, ...,   0,   0,   0],\n",
       "       [220,  44,  97, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee11468",
   "metadata": {},
   "source": [
    "### Creating TensorFlow Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1deeed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 16, input_length=max_len))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(48, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f08b7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 16)           136528    \n",
      "                                                                 \n",
      " global_average_pooling1d_5   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 24)                408       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 48)                1200      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 49        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,185\n",
      "Trainable params: 138,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6811f657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028DC3E83DC0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028DC3E83DC0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028DC3B8EAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028DC3B8EAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "230/230 - 1s - loss: 0.6925 - accuracy: 0.5164 - val_loss: 0.6986 - val_accuracy: 0.4440 - 1s/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "230/230 - 0s - loss: 0.6901 - accuracy: 0.5348 - val_loss: 0.7131 - val_accuracy: 0.4420 - 413ms/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "230/230 - 0s - loss: 0.6779 - accuracy: 0.5684 - val_loss: 0.7216 - val_accuracy: 0.4522 - 430ms/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "230/230 - 0s - loss: 0.6404 - accuracy: 0.6348 - val_loss: 0.7187 - val_accuracy: 0.5310 - 424ms/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "230/230 - 0s - loss: 0.5640 - accuracy: 0.7080 - val_loss: 0.7955 - val_accuracy: 0.5081 - 451ms/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "230/230 - 0s - loss: 0.5022 - accuracy: 0.7540 - val_loss: 0.8667 - val_accuracy: 0.5209 - 443ms/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "230/230 - 0s - loss: 0.4525 - accuracy: 0.7827 - val_loss: 0.9408 - val_accuracy: 0.5107 - 459ms/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "230/230 - 0s - loss: 0.4278 - accuracy: 0.7900 - val_loss: 0.9967 - val_accuracy: 0.5051 - 431ms/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "230/230 - 0s - loss: 0.3903 - accuracy: 0.8132 - val_loss: 1.0537 - val_accuracy: 0.5198 - 428ms/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "230/230 - 0s - loss: 0.3698 - accuracy: 0.8230 - val_loss: 1.1660 - val_accuracy: 0.4812 - 425ms/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "230/230 - 0s - loss: 0.3498 - accuracy: 0.8314 - val_loss: 1.2193 - val_accuracy: 0.4893 - 433ms/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "230/230 - 0s - loss: 0.3298 - accuracy: 0.8400 - val_loss: 1.2302 - val_accuracy: 0.5076 - 445ms/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "230/230 - 0s - loss: 0.3229 - accuracy: 0.8431 - val_loss: 1.4194 - val_accuracy: 0.4725 - 422ms/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "230/230 - 0s - loss: 0.3150 - accuracy: 0.8445 - val_loss: 1.2770 - val_accuracy: 0.5224 - 430ms/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "230/230 - 0s - loss: 0.3055 - accuracy: 0.8494 - val_loss: 1.4385 - val_accuracy: 0.4751 - 436ms/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "230/230 - 0s - loss: 0.2961 - accuracy: 0.8514 - val_loss: 1.3959 - val_accuracy: 0.4980 - 428ms/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "230/230 - 0s - loss: 0.2883 - accuracy: 0.8566 - val_loss: 1.4457 - val_accuracy: 0.4934 - 435ms/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "230/230 - 0s - loss: 0.2871 - accuracy: 0.8573 - val_loss: 1.5527 - val_accuracy: 0.4736 - 412ms/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "230/230 - 0s - loss: 0.2774 - accuracy: 0.8642 - val_loss: 1.5545 - val_accuracy: 0.4863 - 431ms/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "230/230 - 0s - loss: 0.2634 - accuracy: 0.8703 - val_loss: 1.4953 - val_accuracy: 0.5234 - 414ms/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "230/230 - 0s - loss: 0.2684 - accuracy: 0.8665 - val_loss: 1.5866 - val_accuracy: 0.4888 - 416ms/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "230/230 - 0s - loss: 0.2688 - accuracy: 0.8664 - val_loss: 1.5657 - val_accuracy: 0.5086 - 432ms/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "230/230 - 0s - loss: 0.2531 - accuracy: 0.8745 - val_loss: 1.5931 - val_accuracy: 0.5290 - 402ms/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "230/230 - 0s - loss: 0.2559 - accuracy: 0.8739 - val_loss: 1.6780 - val_accuracy: 0.4995 - 449ms/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "230/230 - 0s - loss: 0.2515 - accuracy: 0.8725 - val_loss: 1.7233 - val_accuracy: 0.4959 - 420ms/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "230/230 - 0s - loss: 0.2488 - accuracy: 0.8764 - val_loss: 1.7128 - val_accuracy: 0.5051 - 428ms/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "230/230 - 0s - loss: 0.2393 - accuracy: 0.8756 - val_loss: 1.7390 - val_accuracy: 0.5107 - 451ms/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "230/230 - 0s - loss: 0.2442 - accuracy: 0.8811 - val_loss: 1.7272 - val_accuracy: 0.5264 - 442ms/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "230/230 - 0s - loss: 0.2522 - accuracy: 0.8769 - val_loss: 1.8621 - val_accuracy: 0.4842 - 436ms/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "230/230 - 0s - loss: 0.2352 - accuracy: 0.8803 - val_loss: 1.8886 - val_accuracy: 0.4990 - 424ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28dc2ab14f0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded, train[\"Positive_Change\"], epochs=30, validation_data=(test_padded, test[\"Positive_Change\"]), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ab5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028DC106A790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028DC106A790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bcf6c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred = []\n",
    "for i in pred:\n",
    "    real_pred.append(1 if i > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "75acf022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64b1a1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4989827060020346"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(real_pred, test[\"Positive_Change\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aee8df",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4516939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data, padded, model):\n",
    "    days = []\n",
    "    correct_proportion = []\n",
    "    pred_sum = []\n",
    "    \n",
    "    for date in test_data.Date.unique():\n",
    "        days.append(date)\n",
    "        \n",
    "        day = test_data[test_data[\"Date\"] == date]\n",
    "                \n",
    "        day_padded = padded[day.index[0]:day.index[-1]+1]\n",
    "        \n",
    "        pred = model.predict(day_padded)\n",
    "        \n",
    "        real_pred = []\n",
    "        for i in pred:\n",
    "            real_pred.append(1 if i > 0.5 else 0)\n",
    "            \n",
    "        exp = list(day[\"Positive_Change\"])[0]\n",
    "        pred_sum.append(sum(real_pred))\n",
    "        correct_count = 0\n",
    "        for i in real_pred:\n",
    "            if(i == exp):\n",
    "                correct_count += 1\n",
    "        correct_proportion.append(correct_count/len(real_pred))\n",
    "    return pd.DataFrame({\"Date\":days, \"Correct_Proportion\":correct_proportion, \"Prediction_Sum\":pred_sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd5a1e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Correct_Proportion</th>\n",
       "      <th>Prediction_Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-03-13</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2015-07-10</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2015-04-29</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2014-10-30</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Correct_Proportion  Prediction_Sum\n",
       "0   2016-05-12            0.285714               5\n",
       "1   2014-03-13            0.250000               3\n",
       "2   2019-01-08            0.500000               1\n",
       "3   2022-03-28            1.000000               3\n",
       "4   2015-04-27            0.500000               2\n",
       "..         ...                 ...             ...\n",
       "162 2015-07-10            0.538462               7\n",
       "163 2015-04-29            0.426230              35\n",
       "164 2018-08-29            0.500000               2\n",
       "165 2018-01-05            0.555556               5\n",
       "166 2014-10-30            0.400000               2\n",
       "\n",
       "[167 rows x 3 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(test, test_padded, model)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e4d76188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.Correct_Proportion.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
